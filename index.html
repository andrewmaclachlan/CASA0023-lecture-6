<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Remotely Sensing Cities and Environments</title>
    <meta charset="utf-8" />
    <meta name="author" content="Andy MacLachlan" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies-fonts.css" rel="stylesheet" />
    <script src="libs/js-cookie/js.cookie.js"></script>
    <script src="libs/peerjs/peerjs.min.js"></script>
    <script src="libs/tiny.toast/toast.min.js"></script>
    <link href="libs/xaringanExtra-broadcast/broadcast.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-broadcast/broadcast.js"></script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":false}) })</script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"x2f2cdb47da944a9bedbe657cae15c6a","expires":1}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-progressBar/progress-bar.js"></script>
    <head>
    <link rel="apple-touch-icon" sizes="180x180" href="assets/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon-16x16.png">
    <link rel="manifest" href="assets/site.webmanifest">
    <link rel="mask-icon" href="assets/safari-pinned-tab.svg" color="#5bbad5">
    </head>
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: center, title-slide, middle

background-image: url("img/CASA_Logo_no_text_trans_17.png")
background-size: cover
background-position: center


&lt;style&gt;
.title-slide .remark-slide-number {
  display: none;
}
&lt;/style&gt;



# Remotely Sensing Cities and Environments

### Lecture 6: Classification

### 02/02/2022 (updated: 06/02/2023)

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M511.6 36.86l-64 415.1c-1.5 9.734-7.375 18.22-15.97 23.05c-4.844 2.719-10.27 4.097-15.68 4.097c-4.188 0-8.319-.8154-12.29-2.472l-122.6-51.1l-50.86 76.29C226.3 508.5 219.8 512 212.8 512C201.3 512 192 502.7 192 491.2v-96.18c0-7.115 2.372-14.03 6.742-19.64L416 96l-293.7 264.3L19.69 317.5C8.438 312.8 .8125 302.2 .0625 289.1s5.469-23.72 16.06-29.77l448-255.1c10.69-6.109 23.88-5.547 34 1.406S513.5 24.72 511.6 36.86z"/></svg> [a.maclachlan@ucl.ac.uk](mailto:a.maclachlan@ucl.ac.uk)
<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M459.4 151.7c.325 4.548 .325 9.097 .325 13.65 0 138.7-105.6 298.6-298.6 298.6-59.45 0-114.7-17.22-161.1-47.11 8.447 .974 16.57 1.299 25.34 1.299 49.06 0 94.21-16.57 130.3-44.83-46.13-.975-84.79-31.19-98.11-72.77 6.498 .974 12.99 1.624 19.82 1.624 9.421 0 18.84-1.3 27.61-3.573-48.08-9.747-84.14-51.98-84.14-102.1v-1.299c13.97 7.797 30.21 12.67 47.43 13.32-28.26-18.84-46.78-51.01-46.78-87.39 0-19.49 5.197-37.36 14.29-52.95 51.65 63.67 129.3 105.3 216.4 109.8-1.624-7.797-2.599-15.92-2.599-24.04 0-57.83 46.78-104.9 104.9-104.9 30.21 0 57.5 12.67 76.67 33.14 23.72-4.548 46.46-13.32 66.6-25.34-7.798 24.37-24.37 44.83-46.13 57.83 21.12-2.273 41.58-8.122 60.43-16.24-14.29 20.79-32.16 39.31-52.63 54.25z"/></svg> [andymaclachlan](https://twitter.com/andymaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> [andrewmaclachlan](https://github.com/andrewmaclachlan)
<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M168.3 499.2C116.1 435 0 279.4 0 192C0 85.96 85.96 0 192 0C298 0 384 85.96 384 192C384 279.4 267 435 215.7 499.2C203.4 514.5 180.6 514.5 168.3 499.2H168.3zM192 256C227.3 256 256 227.3 256 192C256 156.7 227.3 128 192 128C156.7 128 128 156.7 128 192C128 227.3 156.7 256 192 256z"/></svg> [Centre for Advanced Spatial Analysis, UCL](https://www.ucl.ac.uk/bartlett/casa/)
<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#562457;overflow:visible;position:relative;"><path d="M88 304H80V256H88C101.3 256 112 266.7 112 280C112 293.3 101.3 304 88 304zM192 256H200C208.8 256 216 263.2 216 272V336C216 344.8 208.8 352 200 352H192V256zM224 0V128C224 145.7 238.3 160 256 160H384V448C384 483.3 355.3 512 320 512H64C28.65 512 0 483.3 0 448V64C0 28.65 28.65 0 64 0H224zM64 224C55.16 224 48 231.2 48 240V368C48 376.8 55.16 384 64 384C72.84 384 80 376.8 80 368V336H88C118.9 336 144 310.9 144 280C144 249.1 118.9 224 88 224H64zM160 368C160 376.8 167.2 384 176 384H200C226.5 384 248 362.5 248 336V272C248 245.5 226.5 224 200 224H176C167.2 224 160 231.2 160 240V368zM288 224C279.2 224 272 231.2 272 240V368C272 376.8 279.2 384 288 384C296.8 384 304 376.8 304 368V320H336C344.8 320 352 312.8 352 304C352 295.2 344.8 288 336 288H304V256H336C344.8 256 352 248.8 352 240C352 231.2 344.8 224 336 224H288zM256 0L384 128H256V0z"/></svg> [PDF presentation](https://github.com/andrewmaclachlan/CASA0023-lecture-6/blob/main/index.pdf)


&lt;a href="https://github.com/andrewmaclachlan" class="github-corner" aria-label="View source on GitHub"&gt;&lt;svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"&gt;&lt;path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"&gt;&lt;/path&gt;&lt;path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"&gt;&lt;/path&gt;&lt;path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}&lt;/style&gt;

---

<style>.xe__progress-bar__container {
  bottom:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #0051BA;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

# How to use the lectures



- Slides are made with [xaringan](https://slides.yihui.org/xaringan/#1)

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M500.3 443.7l-119.7-119.7c27.22-40.41 40.65-90.9 33.46-144.7C401.8 87.79 326.8 13.32 235.2 1.723C99.01-15.51-15.51 99.01 1.724 235.2c11.6 91.64 86.08 166.7 177.6 178.9c53.8 7.189 104.3-6.236 144.7-33.46l119.7 119.7c15.62 15.62 40.95 15.62 56.57 0C515.9 484.7 515.9 459.3 500.3 443.7zM79.1 208c0-70.58 57.42-128 128-128s128 57.42 128 128c0 70.58-57.42 128-128 128S79.1 278.6 79.1 208z"/></svg> In the bottom left there is a search tool which will search all content of presentation

- Control + F will also search 

- Press enter to move to the next result 

- <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M421.7 220.3L188.5 453.4L154.6 419.5L158.1 416H112C103.2 416 96 408.8 96 400V353.9L92.51 357.4C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6L137.6 429.6C143.1 427.7 149.8 424.2 154.6 419.5L188.5 453.4C178.1 463.8 165.2 471.5 151.1 475.6L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1C.8198 498.8-1.502 489.7 .976 481.2L36.37 360.9C40.53 346.8 48.16 333.9 58.57 323.5L291.7 90.34L421.7 220.3zM492.7 58.75C517.7 83.74 517.7 124.3 492.7 149.3L444.3 197.7L314.3 67.72L362.7 19.32C387.7-5.678 428.3-5.678 453.3 19.32L492.7 58.75z"/></svg> In the top right let's you draw on the slides, although these aren't saved.

- Pressing the letter `o` (for overview) will allow you to see an overview of the whole presentation and go to a slide

- Alternatively just typing the slide number e.g. 10 on the website will take you to that slide

- Pressing alt+F will fit the slide to the screen, this is useful if you have resized the window and have another open - side by side. 

<div>
<style type="text/css">.xaringan-extra-logo {
width: 50px;
height: 128px;
z-index: 0;
background-image: url(img/casa_logo.jpg);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:2em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>
---
# Lecture outline

.pull-left[

### Part 1: Review of how classified data is used


### Part 2: How to classify remotely sensed data

]

.pull-right[
&lt;img src="img/satellite.png" width="100%" /&gt;
.small[Source:[Original from the British Library. Digitally enhanced by rawpixel.](https://www.rawpixel.com/image/571789/solar-generator-vintage-style)
]]

---
class: inverse, center, middle

# Let's look back at last week and see how some studies used classfied data


---
# Urban expansion 

**Sensor**

* Landsat

&lt;img src="img/urban_area.png" width="35%" style="display: block; margin: auto;" /&gt;
.small[Figure 2. Urban expansion within the Perth Metropolitan Region (PMR) between 1990 and 2015. Vast urban growth has been observed in PMR with graduating colours exhibiting outward expansion (a); (b) and (c) exhibit static snapshots of urban extent from 2000 (b) and 2015 (c); whilst (d) depicts percentage of urban change per subnational administrative boundary (Local Government Area; LGA).Source:[MacLachlan et al. 2017](https://www.mdpi.com/2073-445X/6/1/9)
]

---

# Air pollution and LULC

**Sensors**

* Sentinel-3 Sea and Land Surface Temperature
* Sentinel-5 Precursor Major Air Pollutants

LULC transformation on air pollution, increase MAP (Major Air Pollutants) and LST 

* Used regression...

* Honeycombing - hex grids for different sensor data

&lt;img src="img/LST_honeycombing.jpg" width="60%" style="display: block; margin: auto;" /&gt;
.small[Fig. 2. The classified honeycomb dataset for LST, PM₂.₅, SO₂, NO₂, CO, and O₃..Source:[Fuldalu and Alta, 2021](https://www.sciencedirect.com/science/article/pii/S2212095521001887?casa_token=0kyJ1dZmkm0AAAAA:syu0WnpPpsCKiY6PiBfzkf2epGa5uldthCpOt1Hey9_pmOF_uel1WpuYECTvF0jr3uzcRCrbd5k#f0005)
]

---
# Air pollution and LULC 2


&gt; There is no doubt that the LULC distribution significantly affects the MAP and LST. Therefore, to determine this relationship the latest LULC distribution shape-file was acquired from the National Cartographic Center of Iran (NCC)

--

&gt; To figure out the impact of LULC on the LST and MAP (Major Air Pollutants) the following statistical comparison perform, the LULC was chosen as an independent variable whereas LST, PM₂.₅, SO₂, NO₂, CO, and O₃ are considered as dependent variables


Although this wasn't used in regression...that was just for the scatter plots of the variables...

But we have classified data (or we might) from a national center 
  * although no data is given
  * no accuracy or method provided

---

# Urban green spaces

&gt; Our results show that the techniques are hybrid methods (37 cases), followed by object-based image analysis (29 cases), land cover indices (20 cases) and fraction methods (16 cases) 

Inventory and assessment (Inv_Ass);Biomass and carbon (BC);Change detection (CD); Ecosystem services (ES):Overall UGSs mapping (OUGS);Species mapping (Spe);Three-dimensional modeling (TDM).

&lt;img src="img/UGS.jpg" width="60%" style="display: block; margin: auto;" /&gt;
.small[Fig. 2. Different techniques to characterize UGSs: (a) frequency of use of techniques according to type of remotely-sensed data, and (b) frequency of use of techniques according to application area. Source:[Shahtahmassebi et al. 2021](https://www.sciencedirect.com/science/article/pii/S1618866720307639?casa_token=ZrACATZktIAAAAAA:9bCBg0pBWBsIPmYMufywYK54cyPXoImsgNxQCN_JBR2zUQ50mvnKHcKZ9CnB2ywCNNsOCw-tpBU#!)]

---

# Urban green spaces 2

&lt;img src="img/UGS_objectives.jpg" width="75%" style="display: block; margin: auto;" /&gt;
.small[Fig. 2. Different techniques to characterize UGSs: (a) frequency of use of techniques according to type of remotely-sensed data, and (b) frequency of use of techniques according to application area. Source:[Shahtahmassebi et al. 2021](https://www.sciencedirect.com/science/article/pii/S1618866720307639?casa_token=ZrACATZktIAAAAAA:9bCBg0pBWBsIPmYMufywYK54cyPXoImsgNxQCN_JBR2zUQ50mvnKHcKZ9CnB2ywCNNsOCw-tpBU#!)]


---
# Monitoring forests + illegal logging 3

&gt; "a more generalized feature space"

* Feature space = scattergram of two bands (or things that have been made into bands)

* Can be used for very basic classification - selecting the values that represent land cover

.pull-left[
&lt;img src="img/Multi_Hyper-spectral_Image_feature_space.svg" width="100%" style="display: block; margin: auto;" /&gt;
.small[Feature space. Source:[Wikimedia commons 2022](https://commons.wikimedia.org/wiki/File:Multi_Hyper-spectral_Image_feature_space.svg)
]
]

.pull-right[
&lt;img src="img/Spectral-curves-scatter-plot.png" width="80%" style="display: block; margin: auto;" /&gt;
.small[Spectral curves on the scatter plot. Source:[50northspatial](http://www.50northspatial.org/n-dimensional-spectral-feature-space-envi/)
]
]
---
# Monitoring forests + illegal logging 4

* Training data (in supervised machine learning)

&gt; Training data to relate to the Landsat metrics were derived from image interpretation methods, including mapping of crown/no crown categories using very high spatial resolution data such as Quickbird imagery, existing percent tree cover layers derived from Landsat data (29), and global MODIS percent tree cover (30), rescaled using the higher spatial resolution percent tree cover data sets

&lt;img src="img/training_data.png" width="80%" style="display: block; margin: auto;" /&gt;
.small[REMAP method. Source:[UN-SPIDER](https://www.un-spider.org/news-and-events/news/new-online-remote-sensing-application-land-cover-classification-and-monitoring)
]

---

# Monitoring forests + illegal logging 5

* Classification (supervised or unsupervised)

&gt; Decision trees are hierarchical classifiers that predict class membership by recursively partitioning a data set into more homogeneous or less varying subsets, referred to as nodes

&lt;img src="img/Hansen_forest_change.jpeg" width="50%" style="display: block; margin: auto;" /&gt;

.small[FIG. 2 Regional subsets of 2000 tree cover and 2000 to 2012 forest loss and gain.(A) Paraguay, centered at 21.9°S, 59.8°W; (B) Indonesia, centered at 0.4°S, 101.5°E; (C) the United States, centered at 33.8°N, 93.3°W; and (D) Russia, centered at 62.1°N, 123.4°E. Source:[Hansen et al. 2013](https://www.science.org/doi/10.1126/science.1244693)
]

Used in Brazil to [target illegal logging]( https://news.mongabay.com/2019/04/how-a-sheriff-in-brazil-is-using-satellites-to-stop-deforestation/) 


---
# Forest fires

* Dates back to the most cited paper on the topic 
    - "Application of remote sensing and geographic information systems to forest fire hazard mapping", Chuvieco and Congalton 1989. 

.pull-left[  
  Used:
  * **Sensor** Landsat TM 1984
  * vegetation, elevation, slope, aspect and road/ house proxmity = fire hazard map compared to burned map from Landsat
  * Did a weighted overlay of the layers - giving hazard value of 0 to 255, some layers had assigned values (e.g. aspect of 90-180 a value of 0)
  * Vegetation was from a classified Landsat TM image - classified 16 categories 
  * No accuracy assessment
  * I assume the manually delineated the burned area pixels 
]
.pull-right[
&lt;img src="img/hazard.png" width="100%" /&gt;
.small[Source:[Chuvieco and Congalton 1989](https://reader.elsevier.com/reader/sd/pii/0034425789900230?token=3F5F9030CFCBBA7544083535303388C8CC1F2D5496F0FFBC273C3673EBFED7B66B2FCAD3EE3B7A6441301FDDAAC7E659&amp;originRegion=eu-west-1&amp;originCreation=20220527153008)
]]

---
class: inverse, center, middle

# In some form all these studies extracted Land Cover from EO data

--

## But how can we do that

---

class: inverse, center, middle

# How do you do that given some imagery ?


.pull-left[

&lt;img src="img/landsat1.jpg" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[NASA, acquired April 23, 1984](https://landsat.visibleearth.nasa.gov/view.php?id=89836)
]]

.pull-right[

&lt;img src="img/landsat2.jpg" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[NASA, acquired July 20, 2016](https://landsat.visibleearth.nasa.gov/view.php?id=89836)
]]


### inductive learning = given context we can use experience to make judgement

---

# Expert Systems

&gt; a system that uses human knowledge to solve problems that normally require human intelligence 

&lt;img src="img/expert_system.jfif" width="80%" style="display: block; margin: auto;" /&gt;
.small[Source:[Aftab Alam](https://www.quora.com/What-is-a-knowledge-based-system-in-the-context-of-artificial-intelligence)
]

* Knowledge Base = Rules of thumb, not always correct

* Inference Engine = Process of reaching a conclusion and the expert system is implemented


This is different to an algorithmic approach = code to solve a solution and when the problem changes so does the code. See Jensen p.433

---

# Expert Systems 2

The problem is how can a computer replicate human knowledge...

.pull-left[
**Q: Tell a computer how you arrived at the decision to wear the clothes you have on today or what you had for lunch yesterday** ?

You might try and represent your knowledge through a series of decisions = **knowledge representation through a decision tree**

If you collected data on this you might be able to draw some conclusions...

]

.pull-right[


&lt;img src="img/decision_tree.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[From the diameter and height of a tree trunk, we must determine if it's an Apple, Cherry, or Oak tree. Source:[Machine Learning University explain](https://mlu-explain.github.io/)
]]

---

class: inverse, center, middle

# Machine learning = science of computer modeling of learning process

--

## When humans have some generalizations we can reach a logical assumption or conclusion = inductive learning.

--


## Machine learning this is a search through all the data to explain the input data and can be used on new input data

---

class: inverse, center, middle


# What city am i in?


### Population of 5.3 million
### Median house price $1,116,219
### Hemisphere: Southern
### Continent: Australia 
### Landmark: Opera house

---

# Decision and classification trees (CART)

* When we create a decision tree the final leaves might be a mixture of the categories = **impure**

.pull-left[

* We quantify this with the Gini Impurity:
  * 1-(probability of yes)^2-(the probability of no)^2
  * weighted based on numbers 

* The one with the lowest impurity goes at the top of the tree to start the decision making...**the root**

* We then use the Gini impurity at each **branch** to split the nodes further 

* Once we don't need to split these turn into **leaves** and the output has **the most votes**

]

.pull-right[


&lt;img src="img/decision_tree2.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[StatQuest](https://www.youtube.com/watch?v=_L39rN6gz7Y)
]]

---
class: inverse, center, middle


# Someone new comes along ...run them (or the data) through the tree

---

# Decision and classification trees (CART) 2

What if we have a leaf with just one person or one pixel value?  = **overfitting**

&lt;img src="img/bias.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Seema Singh](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)]


---


# Decision and classification trees (CART) 3

We can either:
* limit how trees grow (e.g. a minimum number of pixels in a leaf)
* prune the tree:
  * calculate the sum of the squared residuals (SSR, like linear regression) for each leaf
  * Sum for the tree
  * Tree score = SSR + tree penalty (alpha) * T (number of leaves) 

&lt;img src="img/decision_tree_prune.PNG" width="45%" style="display: block; margin: auto;" /&gt;
.small[Source:[StatQuest](https://www.youtube.com/watch?v=D0efHEJsfHo)
]

Alpha: use a full size tree (with **all** data), start with a value of 0 then increase until pruning gives lower tree score. 

Different alpha gives us different sizes of tree that we can test with cross validation. 
---

# Regression trees

What if linear regression doesn't fit the data? ...but we still wanted a numeric value

.pull-left[

&lt;img src="img/regression_tree.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[StatQuest](https://www.youtube.com/watch?v=g9c66TUylZ4)
]]

.pull-right[
 
We divide this up into sections based on thresholds (nodes) and calculate the sum of the squared residuals...

We can then check the SSR for different thresholds...**the one with the lowest SSR is the root of the tree to start with**...then repeat

To prevent over fitting we can set **a minimum number of observations before splitting the data again**.
]
 
---

# Regression trees 2

&lt;img src="img/SSR.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[StatQuest](https://www.youtube.com/watch?v=g9c66TUylZ4)
]
---

# Regression trees 3

* We can do this with many predictor variables...we try different thresholds and calculate the SSR. 

* The best SSR value **across all variables becomes the root**. 

* Each leaf **is a numeric value** not category like in classification trees. 

---

# Random Forests

.pull-left[

* Grow many classification decision trees - **Many better than one**
* Each tree classifies a pixel - it votes for that class, most votes is the winner
* For each tree about 70% of the training data is used, 30% is left out of the bag (OOB)
* Or bootstrap [re-sampling by replacement](https://andrewmaclachlan.github.io/CASA0005repo_20202021/gwr-and-spatially-lagged-regression.html) can be used also termed "bagging"
* One tree uses the whole dataset
* Variables used per tree varies - subset of variables (square root of all of them) is considered at each split of the tree


]

.pull-right[
&lt;img src="img/random_forest.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Rosaria Silipo](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)
]]

???

we take the original dataset and select random data points from within it, but in order to keep it the same size as the original dataset some records are duplicated
---

# Random Forests 2


.pull-left[
* No pruning - trees can do down to largest extent

* **Out of Bag Error**
  * all trees that didn't have the values (e.g. rows in the data) in
  * average prediction error - number of correct predicted/total
  
* Validation data
  * different from OOB
  * never included within the decision trees
  
]

.pull-right[
&lt;img src="img/OOB.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Navnina Bhatia](https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710)
]]

  
---
class: inverse, center, middle

# How do we apply this to imagery

&lt;img src="img/landsat2.jpg" width="85%" style="display: block; margin: auto;" /&gt;
.small[Source:[NASA, acquired July 20, 2016](https://landsat.visibleearth.nasa.gov/view.php?id=89836)
]
---
# Classification trends

&lt;img src="img/image-classification-timeline3.png" width="85%" style="display: block; margin: auto;" /&gt;
.small[Source:[GISGeography](https://gisgeography.com/image-classification-techniques-remote-sensing/)
]
---

# Image classification 


* Turn every pixel in the image into one of a pre-defined categorical classification

* Either supervised or unsupervised classification procedure:


.pull-left[

**supervised**

* Pattern recognition or machine learning 
* Classifier learns patterns in the data
* Uses that to place labels onto new data
* Pattern vector is used to classify the image

Usually pixels treated in isolation but as we have seen - contextual (neighboring pixels), objects (polygons), texture


]


.pull-right[

**unsupervised**

* Identify of land cover classes aren't know a priori (before)
* Tell them computer to cluster based on info it has (e.g. bands)
* Label the clusters

]

---

class: inverse, center, middle

# There are *generic* machine learning algorithms and remote sensing specific ones*


---

# Unsupervised 


Usually referred to as clustering / also k-means:

* Place points randomly or uniformly across spectral feature space or across the first PCA
  * Set the radius in spectral feature space at which new cluster to new started 
  * Spectral distance to merge (within they are the same)
  * Number of pixels to be considered before merging
  * Max number of clusters
  * Clusters migrate over time - see Jensen page 404.
  
Repeat until N iterations or no allocations of pixels left.


&lt;img src="img/cluster_means.png" width="45%" style="display: block; margin: auto;" /&gt;
.small[Source:[Yuting Wan](https://www.researchgate.net/figure/Encoding-strategy-for-the-EAs-based-remote-sensing-image-clustering-approaches_fig2_320378302)
]


---

# Unsupervised 2

**ISODATA**

.pull-left[
Same as k-means but adds:
  * Any clusters have so few pixels = meaningless 
  * Clusters are so close they can be merged 
  * Clusters can be split - elongated clusters in feature space
  
Typically inputs can include:
  * max clusters
  * Max % pixels of class values that can be unchanged - stops
  * Max times of iterations
  * Min pixels per cluster
  * Max standard deviation - then split the cluster
  * Min distance between clusters (3)

]

.pull-right[

&lt;img src="img/isodata.PNG" width="50%" style="display: block; margin: auto;" /&gt;
.small[Source:[Jensen 2016 p.409 / Muhammad Zulkarnain Abdul Rahman](https://people.utm.my/nurulhazrina/files/2015/05/L12-Unsupervised-classification.pdf)
]


]
---
# Unsupervised 3

## Cluster busting

.pull-left[
ISODATA can create lots of clusters and it's difficult to assign meaning (e.g. landcover)

  * Two types of landcover in the pixel 
  * distribution of mean vectors not good enough to differentiate them

Let's **bust those clusters!**
  * Take the incorrect or difficult to label ones
  * Mask them
  * Perform a separate classification 
  * Repeat 
]

.pull-right[
&lt;img src="img/ISODATA2.PNG" width="80%" style="display: block; margin: auto;" /&gt;
.small[Source:[Jensen 2016 p.409 / Muhammad Zulkarnain Abdul Rahman](https://people.utm.my/nurulhazrina/files/2015/05/L12-Unsupervised-classification.pdf)
]

]
---

class: inverse, center, middle

# How does supervised differ from unsupervised...?

---
# Supervised 

Parametric (normal distribution) or non parametric (not normal)? 

I would call most of these **"classical"** or **"traditional"** classifiers as they aren't used *much* now

.pull-left[
**Parametric**
* Maximum likelihood

]


.pull-right[
**Non-parametric**

* Density slicing
* Parallelpiped
* Minimum distance to mean 
* Nearest neighbor 
* Neural networks
* Machine learning / expert systems*

]
  
* More recent work uses machine learning / expert systems(e.g. Support Vector Machine, Neural Networks   ) or spectral mixture analysis
  
---
  
# Supervised 2
 
.pull-left[

Same process for all:

  * class definition
  * pre-processing
  * training
  * pixel assignment
  * accuracy assessment

] 
 
  
.pull-right[
&lt;img src="img/supervised-diagram.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[GIS Geography](https://gisgeography.com/supervised-unsupervised-classification-arcgis/)
]
]

---
class: inverse, center, middle

# Here we will focus on two common methods - maximum likelihood and support vector machines

## We have already covered decision trees and random forests

## But...

---

# Supervised 3

An approach to select a classifier...in most cases training samples will overlap...unless you select **spectrally pure endmembers** or use a **spectral library**.  

.pull-left[

&lt;img src="img/supervised-classification-algorithm-selection-en.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Pavel Ukrainski](http://www.50northspatial.org/pick-best-supervised-classification-method/)
]

] 
 
  
.pull-right[
&lt;img src="img/parallelepiped.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Pavel Ukrainski](http://www.50northspatial.org/pick-best-supervised-classification-method/)]
]


---
# Maximum likelihood


.pull-left[
**Basics**

* Decision rule classifier 
* Uses probability 
* Takes the image and assigns pixel to the most probable land cover type.
* You can set a probability threshold which means if a pixel is below it = no classification. 

] 
 
  
.pull-right[
&lt;img src="img/029.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Machine Learning Mastery](https://machinelearningmastery.com/probability-density-estimation/)]
]



---

# Maximum likelihood 2


.pull-left[
**Specifics**

* From histogram to probability density function
  * mean and standard deviation of training data
* In imagery this is *n* dimensional multivariate normal density function - see Jensen p.399 for equation. 
* Each pixel from image data is passed to the maximum likelihood rule &gt; assigns landover to the largest product.



] 
  
.pull-right[

&lt;img src="img/maximum.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Núñez et al. 2018 High-Resolution Satellite Imagery Classification for Urban Form Detection](https://www.intechopen.com/chapters/64971)]
]

---
class: inverse, center, middle

# Maximum Likelihood allows classification with **prior** pobablity information (e.g. 60% expected to be urban)

## Usually we don't have this though 

---
class: inverse, center, middle

# Terminology alert

## Pattern vector = all the band values per pixel (could include texture etc) 

## Also recall [how we can fuse data from lecture 3](https://andrewmaclachlan.github.io/CASA0023-lecture-3/?panelset2=fusion&amp;panelset3=dn2&amp;panelset4=ratio2&amp;panelset5=pca2#48)

---

# Support Vector Machine (SVM)


.pull-left[


* Simply a linear binary classifier - like logistic regression! 

* Maximum **margin** between two classes of training data

* Points on the boundaries are **support vectors**

* Middle margin is called the **separating hyperplane**

] 
  
.pull-right[

This can be thought of as training data for class a on one side and training data for class b on the other side, with band 1 on the x axis and band 2 on the y axis. 

&lt;img src="img/svm.jpg" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Núñez et al. 2018 High-Resolution Satellite Imagery Classification for Urban Form Detection](https://www.intechopen.com/chapters/64971)]
]

---

# Support Vector Machine (SVM) 2

.pull-left[

* Underlying theory is **structural risk minimisation**
  * Minimise error on unseen data with no assumptions on the distribution 
  
Selectable:
* [Type of kernel](https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/)
* **C** controls training data and decision boundary maximisation plus margin errors. The bigger = narrower margin.
* **Gamma (or Sigma)**  low = big radius for classified points, high = low radius for classified points
  * More on this next week...

] 
  
.pull-right[

* If they aren't linearly separable we can transform the data with the **kernel trick**
  * Apply some function to make them linearly separable 


&lt;img src="img/kernel_trick.png" width="100%" style="display: block; margin: auto;" /&gt;

&lt;img src="img/quadratic_transformation.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Drew Wilimitis](https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f#:~:text=The%20%E2%80%9Ctrick%E2%80%9D%20is%20that%20kernel,the%20data%20by%20these%20transformed)]
]

---
class: inverse, center, middle

# Some considerations...

---
class: inverse, center, middle

# **Do not** just present a study that classifies imagery ...look back at the examples and how else LULC be used

---
# Hard or soft?

.pull-left[
&lt;img src="img/fuzzy.PNG" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Jensen, page 413/Slides](http://web.pdx.edu/~nauna/week7-final.pdf)
]]


.pull-right[
&lt;img src="img/Four-causes-of-mixed-pixels.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Peter Fisher](https://www.researchgate.net/figure/Four-causes-of-mixed-pixels_fig3_242103275)

]]

.small[[A. P. Cracknell, 2010, Review article Synergy in remote sensing-what's in a pixel?](https://www.tandfonline.com/doi/pdf/10.1080/014311698214848?needAccess=true)
]
---
# Pixels or objects?

.pull-left[
&lt;img src="img/OBIA_1.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Jakub Nowosad, supercells](https://jakubnowosad.com/supercells/articles/rgb_vars.html)
]
]
.pull-right[
&lt;img src="img/OBIA.png" width="100%" style="display: block; margin: auto;" /&gt;
.small[Source:[Jakub Nowosad, supercells](https://jakubnowosad.com/supercells/articles/rgb_vars.html)
]
]

&lt;img src="img/OBIA_3.png" width="50%" style="display: block; margin: auto;" /&gt;
.small[Average colour per segment. Source:[Jakub Nowosad, supercells](https://jakubnowosad.com/supercells/articles/rgb_vars.html)
]
---
class: inverse, center, middle

# Pixels or objects?

## This stems from simple linear iterative clustering (SLIC)...which uses k-means clustering to create the superpixels. See [Achanta et al. (2012)](https://pubmed.ncbi.nlm.nih.gov/22641706/)

---
# Blackbox?

&lt;img src="img/blackbox.PNG" width="70%" style="display: block; margin: auto;" /&gt;
.small[Source:[SHEYKHMOUSA et al. 2020 Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review](https://www.researchgate.net/figure/Four-causes-of-mixed-pixels_fig3_242103275)]

---

# Summary 




---
# Papers

https://gisgeography.com/image-classification-techniques-remote-sensing/

https://www.researchgate.net/figure/Land-use-mapping-Maximum-Likehood-Classification-In-this-study-maximum-likelihood_fig1_263471542

Read: https://www.tandfonline.com/doi/full/10.1080/01431160512331314083
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
